{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "import transformers\n",
    "from transformers import TFT5ForConditionalGeneration,T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.6.0', '2.18.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__,tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\revan\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = TFT5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>About the Digital Business Unit at IndusInd:\\n...</td>\n",
       "      <td>IndusInd Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Company Description Sennkarathaaan Food and Be...</td>\n",
       "      <td>Sennkarathaaan Food and Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Job Description\\n\\n A job where you increase t...</td>\n",
       "      <td>AIMonk Labs Private Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job Description\\n\\nSome careers have more impa...</td>\n",
       "      <td>HSBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job Description\\n\\nWe have an opportunity to i...</td>\n",
       "      <td>JPMorganChase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  About the Digital Business Unit at IndusInd:\\n...   \n",
       "1  Company Description Sennkarathaaan Food and Be...   \n",
       "2  Job Description\\n\\n A job where you increase t...   \n",
       "3  Job Description\\n\\nSome careers have more impa...   \n",
       "4  Job Description\\n\\nWe have an opportunity to i...   \n",
       "\n",
       "                        Company Name  \n",
       "0                      IndusInd Bank  \n",
       "1  Sennkarathaaan Food and Beverages  \n",
       "2            AIMonk Labs Private Ltd  \n",
       "3                               HSBC  \n",
       "4                      JPMorganChase  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r\"C:\\Users\\revan\\Downloads\\jobsDetails_dataset.json\",'r') as data:\n",
    "    data = json.load(data)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>About the Digital Business Unit at IndusInd:\\n...</td>\n",
       "      <td>IndusInd Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Company Description Sennkarathaaan Food and Be...</td>\n",
       "      <td>Sennkarathaaan Food and Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Job Description\\n\\n A job where you increase t...</td>\n",
       "      <td>AIMonk Labs Private Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job Description\\n\\nSome careers have more impa...</td>\n",
       "      <td>HSBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job Description\\n\\nWe have an opportunity to i...</td>\n",
       "      <td>JPMorganChase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>Genpact (NYSE: G) is a global professional ser...</td>\n",
       "      <td>Genpact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>Duration- 6monthsLocation- Bangalore/Delhi (Hy...</td>\n",
       "      <td>Elucidata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>About Us: Nextwave Multimedia is a passionate ...</td>\n",
       "      <td>Nextwave Multimedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>Skills:\\nData Analytics, Reporting, SAS, SQL, ...</td>\n",
       "      <td>Covenant Consultants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>At UnitedHealthcare, we’re simplifying the hea...</td>\n",
       "      <td>UnitedHealthcare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Description  \\\n",
       "0     About the Digital Business Unit at IndusInd:\\n...   \n",
       "1     Company Description Sennkarathaaan Food and Be...   \n",
       "2     Job Description\\n\\n A job where you increase t...   \n",
       "3     Job Description\\n\\nSome careers have more impa...   \n",
       "4     Job Description\\n\\nWe have an opportunity to i...   \n",
       "...                                                 ...   \n",
       "5211  Genpact (NYSE: G) is a global professional ser...   \n",
       "5212  Duration- 6monthsLocation- Bangalore/Delhi (Hy...   \n",
       "5213  About Us: Nextwave Multimedia is a passionate ...   \n",
       "5214  Skills:\\nData Analytics, Reporting, SAS, SQL, ...   \n",
       "5215  At UnitedHealthcare, we’re simplifying the hea...   \n",
       "\n",
       "                           Company Name  \n",
       "0                         IndusInd Bank  \n",
       "1     Sennkarathaaan Food and Beverages  \n",
       "2               AIMonk Labs Private Ltd  \n",
       "3                                  HSBC  \n",
       "4                         JPMorganChase  \n",
       "...                                 ...  \n",
       "5211                            Genpact  \n",
       "5212                          Elucidata  \n",
       "5213                Nextwave Multimedia  \n",
       "5214               Covenant Consultants  \n",
       "5215                   UnitedHealthcare  \n",
       "\n",
       "[5216 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n',' ',text)\n",
    "    text = re.sub(r'\\,','',text)\n",
    "    text = re.sub(r'\\(','',text)\n",
    "    text = re.sub(r'\\)','',text)\n",
    "    text = re.sub(r'\\:',' ',text)\n",
    "    text = re.sub(r'/','',text)\n",
    "    text = re.sub(r'-','',text)\n",
    "    text = re.sub(r'\\.','',text)\n",
    "    text = re.sub(r'\\*','',text)\n",
    "    text = re.sub(r'\\\"','',text)\n",
    "    text = re.sub(r\"\\'\",'',text)\n",
    "    text = re.sub(r\"\\+\",'',text)\n",
    "    text = re.sub(r\"\\?\",'',text)\n",
    "    return text\n",
    "\n",
    "def removing_stopwords(text):\n",
    "\n",
    "    text = text.split()\n",
    "    text = [i for i in text if i not in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>About the Digital Business Unit at IndusInd:\\n...</td>\n",
       "      <td>IndusInd Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Company Description Sennkarathaaan Food and Be...</td>\n",
       "      <td>Sennkarathaaan Food and Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Job Description\\n\\n A job where you increase t...</td>\n",
       "      <td>AIMonk Labs Private Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job Description\\n\\nSome careers have more impa...</td>\n",
       "      <td>HSBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job Description\\n\\nWe have an opportunity to i...</td>\n",
       "      <td>JPMorganChase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Company DescriptionOnelab Ventures is a leadin...</td>\n",
       "      <td>Onelab Ventures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Note: By applying to this position you will ha...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Role Overview: As a Data Scientist specializin...</td>\n",
       "      <td>Astria Digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Who we are: -At CitiusTech, we constantly stri...</td>\n",
       "      <td>CitiusTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Experience: ~1.5 to 4 Years\\n\\nLanguage: Pytho...</td>\n",
       "      <td>NIBODHAH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Description  \\\n",
       "0    About the Digital Business Unit at IndusInd:\\n...   \n",
       "1    Company Description Sennkarathaaan Food and Be...   \n",
       "2    Job Description\\n\\n A job where you increase t...   \n",
       "3    Job Description\\n\\nSome careers have more impa...   \n",
       "4    Job Description\\n\\nWe have an opportunity to i...   \n",
       "..                                                 ...   \n",
       "995  Company DescriptionOnelab Ventures is a leadin...   \n",
       "996  Note: By applying to this position you will ha...   \n",
       "997  Role Overview: As a Data Scientist specializin...   \n",
       "998  Who we are: -At CitiusTech, we constantly stri...   \n",
       "999  Experience: ~1.5 to 4 Years\\n\\nLanguage: Pytho...   \n",
       "\n",
       "                          Company Name  \n",
       "0                        IndusInd Bank  \n",
       "1    Sennkarathaaan Food and Beverages  \n",
       "2              AIMonk Labs Private Ltd  \n",
       "3                                 HSBC  \n",
       "4                        JPMorganChase  \n",
       "..                                 ...  \n",
       "995                    Onelab Ventures  \n",
       "996                             Google  \n",
       "997                     Astria Digital  \n",
       "998                         CitiusTech  \n",
       "999                           NIBODHAH  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.iloc[0:1000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revan\\AppData\\Local\\Temp\\ipykernel_13176\\441431256.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Description'] = data['Description'].map(lambda data : removing_stopwords(cleaning_data(data)))\n",
      "C:\\Users\\revan\\AppData\\Local\\Temp\\ipykernel_13176\\441431256.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Company Name'] = data['Company Name'].map(lambda data:removing_stopwords(cleaning_data(data)))\n"
     ]
    }
   ],
   "source": [
    "data['Description'] = data['Description'].map(lambda data : removing_stopwords(cleaning_data(data)))\n",
    "data['Company Name'] = data['Company Name'].map(lambda data:removing_stopwords(cleaning_data(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1629, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_maxlen  = max(len(data['Company Name'].iloc[i].split()) for i in range(len(data)))\n",
    "des_maxlen = max(len(data['Description'].iloc[i].split()) for i in range(len(data)))\n",
    "\n",
    "des_maxlen,com_maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens({'bos_token': '<start>'})\n",
    "\n",
    "input_tokenized_data = [tokenizer.encode(data['Description'].iloc[i],\n",
    "                                         max_length = des_maxlen,\n",
    "                                         padding = 'max_length',\n",
    "                                         truncation=True,\n",
    "                                         return_tensors = 'tf') for i in range(len(data))]\n",
    "\n",
    "\n",
    "output_tokenized_data = [tokenizer.encode(data['Company Name'].iloc[i],\n",
    "                                          max_length = com_maxlen,\n",
    "                                          padding = 'max_length',\n",
    "                                          truncation = True,\n",
    "                                          return_tensors = 'tf') for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_out = []\n",
    "for i in range(len(output_tokenized_data)):\n",
    "    dex = np.zeros_like(output_tokenized_data[i].numpy())\n",
    "    dex[:,:-1] = output_tokenized_data[i].numpy()[:,1:]\n",
    "    dex[:,-1] = 0\n",
    "    decoder_out.append(tensorflow.convert_to_tensor(dex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenized_data = tensorflow.constant([x.numpy()[0] for x in input_tokenized_data])\n",
    "output_tokenized_data = tensorflow.constant([x.numpy()[0] for x in output_tokenized_data])\n",
    "decoder_out = tensorflow.constant([x.numpy()[0] for x in decoder_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokenized_data[0],decoder_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tensorflow.data.Dataset.from_tensor_slices(({\"input_ids\": input_tokenized_data}, {\"labels\": output_tokenized_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenized_data[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tensorflow.data.Dataset.from_tensor_slices(({\"input_ids\":input_tokenized_data},{\"labels\":output_tokenized_data}))\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tensorflow.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenized_data = tokenizer(\n",
    "    data['Description'].tolist(),\n",
    "    max_length=des_maxlen,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    "     ).input_ids\n",
    "\n",
    "output_tokenized_data = tokenizer(\n",
    "    data['Company Name'].tolist(),\n",
    "    max_length=com_maxlen,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    "    ).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Batch 0/1000 - Loss: [7.645852]\n"
     ]
    }
   ],
   "source": [
    "# Prepare shifted output (decoder inputs) for teacher forcing\n",
    "decoder_input_ids = []\n",
    "for tokens in output_tokenized_data:\n",
    "    shifted = tensorflow.concat([tensorflow.constant([tokenizer.pad_token_id]), tokens[:-1]], axis=0)\n",
    "    decoder_input_ids.append(shifted)\n",
    "decoder_input_ids = tensorflow.convert_to_tensor(decoder_input_ids)\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = tensorflow.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss_fn = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training function\n",
    "@tensorflow.function\n",
    "def train_step(input_ids, decoder_input_ids, labels):\n",
    "    with tensorflow.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "    # Backpropagation\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_input_ids = input_tokenized_data[i:i + batch_size]\n",
    "        batch_decoder_input_ids = decoder_input_ids[i:i + batch_size]\n",
    "        batch_labels = output_tokenized_data[i:i + batch_size]\n",
    "\n",
    "        loss = train_step(batch_input_ids, batch_decoder_input_ids, batch_labels)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Batch {i}/{len(data)} - Loss: {loss.numpy()}\")\n",
    "\n",
    "# Inference function\n",
    "def predict_company_name(description):\n",
    "    input_ids = tokenizer.encode(description, return_tensors='tf', truncation=True, max_length=des_maxlen)\n",
    "    outputs = model.generate(input_ids, max_length=com_maxlen)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "# Testing inference on a sample description\n",
    "sample_description = data['Description'].iloc[0]\n",
    "predicted_name = predict_company_name(sample_description)\n",
    "print(f\"Predicted Company Name: {predicted_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = []\n",
    "for tokens in output_tokenized_data:\n",
    "    shifted = tensorflow.concat([tensorflow.constant([tokenizer.pad_token_id]), tokens[:-1]], axis=0)\n",
    "    decoder_input_ids.append(shifted)\n",
    "decoder_input_ids = tensorflow.convert_to_tensor(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=int32, numpy=array([   0,   16, 8655,   77,   26, 2137,    1,    0], dtype=int32)>,\n",
       " <tf.Tensor: shape=(8,), dtype=int32, numpy=array([  16, 8655,   77,   26, 2137,    1,    0,    0], dtype=int32)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_ids[0],output_tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Series.rename() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m     data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob Description\u001b[39m\u001b[38;5;124m'\u001b[39m},inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m---> 23\u001b[0m json_data \u001b[38;5;241m=\u001b[39m \u001b[43mreading_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mrevan\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mjobsDetails_20241106130529.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(json_data\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mreading_json\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m300\u001b[39m]\n\u001b[0;32m     17\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenuine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob Description\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mTypeError\u001b[0m: Series.rename() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def reading_json(path:'str') ->str:\n",
    "\n",
    "    with open(path,'r',encoding='utf-8') as json_file:\n",
    "\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    data = data.iloc[0:600]['Description']\n",
    "\n",
    "    data.drop_duplicates(inplace=True,ignore_index=True)\n",
    "\n",
    "    data = data.iloc[0:300]\n",
    "    data['Job Type'] = \"Genuine\"\n",
    "\n",
    "    data.rename({'Description':'Job Description'},inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "json_data = reading_json(r\"C:\\Users\\revan\\Downloads\\jobsDetails_20241106130529.json\")\n",
    "\n",
    "print(json_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"labtech innovations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "name = input(\"Enter the company name\")\n",
    "\n",
    "name = name.replace(\" \",\"-\")\n",
    "\n",
    "# URL to open\n",
    "url = \"https://www.quickcompany.in/company/search?q=\"+name\n",
    "\n",
    "# Open the URL in the default web browser\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch the website. HTTP Status Code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Input company name\n",
    "name = input(\"Enter the company name: \")\n",
    "\n",
    "# Replace spaces with hyphens\n",
    "name = name.replace(\" \", \"-\")\n",
    "\n",
    "# URL to scrape\n",
    "url = f\"https://www.quickcompany.in/company/search?q={name}\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find relevant data (modify based on website's structure)\n",
    "    company_results = soup.find_all('div', class_='search-results-item')  # Update the class name accordingly\n",
    "    \n",
    "    if company_results:\n",
    "        for idx, company in enumerate(company_results, 1):\n",
    "            # Extract details from each result (example: name, link, etc.)\n",
    "            company_name = company.find('h3').text.strip()  # Example: Find the company name\n",
    "            company_link = company.find('a')['href'].strip()  # Example: Find the link to company details\n",
    "            \n",
    "            print(f\"{idx}. Company Name: {company_name}\")\n",
    "            print(f\"   Link: {company_link}\\n\")\n",
    "    else:\n",
    "        print(\"No companies found!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the website. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Input company name\n",
    "name = input(\"Enter the company name: \")\n",
    "\n",
    "# Replace spaces with hyphens\n",
    "name = name.replace(\" \", \"-\")\n",
    "\n",
    "# URL to scrape\n",
    "url = f\"https://www.quickcompany.in/company/search?q={name}\"\n",
    "\n",
    "# Add headers to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send a GET request with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(\"Website fetched successfully!\")\n",
    "    # Add your scraping logic here\n",
    "else:\n",
    "    print(f\"Failed to fetch the website. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website fetched successfully!\n",
      "No companies found!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Input company name\n",
    "name = input(\"Enter the company name: \")\n",
    "\n",
    "# Replace spaces with hyphens\n",
    "name = name.replace(\" \", \"-\")\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.quickcompany.in/company/search?q=\"+name\n",
    "\n",
    "# Add headers to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send a GET request with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(\"Website fetched successfully!\")\n",
    "    \n",
    "    # Find the search result items (adjust the tag and class based on the website's structure)\n",
    "    company_results = soup.find_all('div', class_='company-item')  # Replace 'search-results-item' with actual class name\n",
    "    \n",
    "    if company_results:\n",
    "        # Open a CSV file to save results\n",
    "        with open('companies.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Company Name', 'Link'])  # Header row\n",
    "            \n",
    "            for idx, company in enumerate(company_results, 1):\n",
    "                # Extract company name\n",
    "                company_name = company.find('h3').text.strip() if company.find('h3') else \"No name found\"\n",
    "                \n",
    "                # Extract link\n",
    "                company_link = company.find('a')['href'].strip() if company.find('a') else \"No link found\"\n",
    "                \n",
    "                # Print company details\n",
    "                print(f\"{idx}. Company Name: {company_name}\")\n",
    "                print(f\"   Link: {company_link}\\n\")\n",
    "\n",
    "                \n",
    "                # Write to CSV\n",
    "                writer.writerow([company_name, company_link])\n",
    "\n",
    "                print(soup.prettify())\n",
    "\n",
    "        \n",
    "        print(\"Data saved to companies.csv\")\n",
    "    else:\n",
    "        print(\"No companies found!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the website. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'capabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Set up Selenium WebDriver\u001b[39;00m\n\u001b[0;32m     16\u001b[0m driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/chromedriver\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the path to your WebDriver\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Open the URL\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n",
      "File \u001b[1;32mc:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[0;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\revan\\anaconda\\envs\\tfod\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:56\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths\n\u001b[1;32m---> 56\u001b[0m browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapabilities\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service\u001b[38;5;241m.\u001b[39mpath\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Input company name\n",
    "name = input(\"Enter the company name: \")\n",
    "\n",
    "# Replace spaces with hyphens\n",
    "name = name.replace(\" \", \"-\")\n",
    "\n",
    "# URL to scrape\n",
    "url = f\"https://www.quickcompany.in/company/search?q={name}\"\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = 'path/to/chromedriver'  # Replace with the path to your WebDriver\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "try:\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(5)  # Adjust sleep time as needed\n",
    "\n",
    "    # Find company results (update the selector as per the structure)\n",
    "    company_results = driver.find_elements(By.CLASS_NAME, 'search-results-item')  # Update the class name\n",
    "\n",
    "    # Open a CSV file to save results\n",
    "    with open('companies.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Company Name', 'Link'])  # Header row\n",
    "\n",
    "        for idx, company in enumerate(company_results, 1):\n",
    "            # Extract company name\n",
    "            company_name = company.find_element(By.TAG_NAME, 'h3').text.strip() if company.find_elements(By.TAG_NAME, 'h3') else \"No name found\"\n",
    "            \n",
    "            # Extract link\n",
    "            company_link = company.find_element(By.TAG_NAME, 'a').get_attribute('href') if company.find_elements(By.TAG_NAME, 'a') else \"No link found\"\n",
    "            \n",
    "            # Print company details\n",
    "            print(f\"{idx}. Company Name: {company_name}\")\n",
    "            print(f\"   Link: {company_link}\\n\")\n",
    "            \n",
    "            # Write to CSV\n",
    "            writer.writerow([company_name, company_link])\n",
    "\n",
    "    print(\"Data saved to companies.csv\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
